random_state = 42
verbose = true
train = false     # train using default hparams
tune = true       # tune hparams
evaluate = true
train_size = 0.8

[model]
vocab_size = 100
embedding_dim = 256
hidden_dim = 64
n_layers = 3
output_dim = 1
padding_idx = 0

[optimizer]
lr = 1e-5
momentum = 0.9
weight_decay = 1e-6

[hparams.optimizer]

[dataloader]
batch_size = 32
num_workers = 0   # > 1 for multiprocessing
pin_memory = true

[trainer]
max_epochs = 10
gradient_clip = 1.0
eval_every_n_epochs = 1
device = "mps"          # "cpu", "cuda" or "mps"

[checkpoint]
path = "data/checkpoints"
mode = "minimize"         # "minimize" or "maximize"

[tuner]
n_trials = 16


[evaluator]
n_bootstraps = 100

[hparams]
[hparams.hidden_dim]
name = "hidden_dim"
choices = [32, 64, 128]
[hparams.n_layers]
name = "n_layers"
low = 1
high = 6
[hparams.lr]
name = "lr"
low = 1e-5
high = 1e-2
log = true
[hparams.weight_decay]
name = "weight_decay"
low = 1e-10
high = 1e-3
log = true
[hparams.momentum]
name = "momentum"
low = 0.9
high = 0.99
